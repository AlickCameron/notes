---
title: "Fitting Simple Linear Regresion Models"
author: "Math 430, Winter 2017"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
mpg <- read.csv(file = "https://github.com/math430-lu/data/raw/master/mpg.csv")
```

# Predicting fuel economy

* **Task:** predict the fuel economy of a vehicle based on its weight

    + i.e. find $\widehat{\beta}_0$ and $\widehat{\beta}_1$ 

$$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1x_i$$

* **Approach:**  minimize the residual sums of squares

$$RSS = \sum_{i=1}^n \left(y_i - b_0 - b_1 x_i \right)^2$$

* This is called least squares (LS) estimation

---

# Linear models in R

`lm` is our workhorse function

```{r}
mod <- lm(mpg ~ weight, data = mpg)
```

* The formula is of the form `response ~ predictor`

* The result is an object of class `lm`

```{r}
names(mod)
```

---

# Linear models in R

You have a few options to the results

1. **Print**: print `mod` to see the estimated regression coefficients

1. **Summary**: `summary(mod)` displays the most useful information about the model

1. **Attributes**: extract the attribute of interest using the `$` operator

---
class: middle

```{r}
summary(mod)
```

---

# Interpreting the slope

---

# Interpreting the intercept

---

# Making predictions

Once we have our estimated regression coefficients, $\hat{\beta}_0$ and $\hat{\beta}_1$, obtaining a prediction is easy.

<br>


**Example** predict the MPG for a car weighing 2,500 lbs

--

$$\widehat{y}=\widehat{\beta}_0+\widehat{\beta}_1(2500)$$

--
<br>
<br>

In R, we use the `predict` function

```{r}
predict(mod, newdata = data.frame(weight = 2500))
```

---

# The full SLR model

* LS only assumes that there is a linear relationship between $x$ and $y$

* Additional assumptions are needed to understand the uncertainty of our predictions

* The SLR model can be written in a few forms

    + $Y_i = \beta_0 + \beta_1x_i + e_i$ where $e\overset{iid}{\sim}\mathcal{N}(0, \sigma^2)$
    
    + $Y_i \overset{iid}{\sim} \mathcal{N}(\beta_0 + \beta_1x_i, \sigma^2)$
    

---
class: middle

![](figs/slr.png)

---

# Regression assumptions

1. **Linearity**: $E(Y|X = x_i) = \beta_0 + \beta_1 x$

1. **Independence**: $e_1, \ldots, e_n$ are independent

1. **Constant error variance**: $Var(e_1) = \cdots = Var(e_n) = \sigma^2$

1. **Normal error terms**: $e \sim \mathcal{N}(0, \sigma^2)$

---

# ML estimation

We cannot obtain an estimate of $\sigma^2$ through LS, so instead we can use maximum likelihood (ML)

<br>

To do this, we simply maximize the likelihood function

$$L(\beta_0, \beta_1, \sigma) = \prod_{i=1}^n f(y_i | x_i, \beta_0, \beta_1, \sigma) = \prod_{i=1}^n \dfrac{1}{\sigma \sqrt{2\pi}} e^{-(y_i - \beta_0 - \beta_1 x_i)/2\sigma^2}$$

<br>

Idea: finding the values of $\beta_0$, $\beta_1$, and $\sigma$ that make our data most likely
---

# ML estimation

It's often easier to work with the log likelihood

$$\ell(\beta_0, \beta_1, \sigma) = \log{L(\beta_0, \beta_1, \sigma)} = \sum_{i=1}^n \log(\sigma) - \dfrac{1}{2}\log(2\pi) - (y_i - \beta_0 - \beta_1 x_i)^2 / 2 \sigma^2$$ 

Taking partial derivatives we find

$\dfrac{\partial \ell}{\partial \beta_0} = \dfrac{1}{\sigma^2} \displaystyle \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)$

$\dfrac{\partial \ell}{\partial \beta_1} = \dfrac{1}{\sigma^2} \displaystyle \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)x_i$

$\dfrac{\partial \ell}{\partial \sigma} = \dfrac{-n}{\sigma} - \dfrac{1}{\sigma^3} \displaystyle \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 = \dfrac{1}{\sigma^2} \left( n\sigma^2 - \displaystyle \sum_{i=1}^n e^2_i \right)$

---

# ML estimation

Setting the derivatives to 0 and solving yields

$$\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x} \qquad \widehat{\beta}_1 = \dfrac{SXY}{SXX} \qquad \widehat{\sigma}^2 = \dfrac{\sum_{i=1}^n e^2_i}{n}$$

* $\widehat{\beta}_0$ and $\widehat{\beta}_1$ are the LS estimates

* The above estimate of $\sigma^2$ is biased, so we must make an adjustment to obtain an unbiased estimator

$$\widehat \sigma^2 = \dfrac{\sum_{i=1}^n e^2_i}{n - 2}$$

---

# Properties of our estimators

* $\widehat{\beta}_0$ and $\widehat{\beta}_1$ are **unbiased estimates** of $\beta_0$ and $\beta_1$

* $\widehat{\beta}_0$ and $\widehat{\beta}_1$ are the *best linear unbiased estimates* (BLUE); that is, they have the smallest variance of all linear unbiased estimates

* $\widehat{\sigma}_\varepsilon$ is an unbiased estimate of $\sigma_\varepsilon$

