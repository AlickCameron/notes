---
title: "Introduction to regression"
author: "Prof. Adam Loy"
date: "Math 430, Winter 2017"
output: 
  ioslides_presentation:
    fig_height: 4
    fig_width: 4
    out_width: 400px
    fig_retina: null
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## What is a statistical model?

- A model is a representation for a particular purpose (Daniel Kaplan)
- A simplification of reality (Cannon et al., 2010)
- A statistical model embodies a set of assumptions concerning the generation of the observed data, and similar data from a larger population (Wikipedia)
- Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful (George Box)

## What is a statistical model?

### Notation
- $Y_i$: **response variable** measured on the $i^{\text{th}}$ *observational unit*, $i=1,\ldots,n$
- $x_{i1}, x_{i2}, \ldots, x_{ik}$: **predictor/explanatory/covariate variables** measured on the $i^{\text{th}}$ *observational unit*

### Statistical model

$$Y_i = f(x_{i1}, x_{i2}, \ldots, x_{ik}) + \varepsilon$$

where

- $f(\cdot)$: a mathematical function
- $\varepsilon$: a random error term

## Why do we model?

1. Prediction

2. Understanding relationships

3. Assessing differences


## Challenges we will face

### Data collection and variable selection

- What data do we need to achieve our goals?
- Can we use data to actually answer our questions?
- What types of variables do we have?
- What variables are actually important?



## Challenges we will face


### Model specification

- What is the "right" model to use? 
- Does this choice help us achieve our goals?
- What assumptions are embedded in our choice?
- Are these assumptions valid?


## Challenges we will face {.columns-2}

### Model specification
<br>
```{r, echo=FALSE}
library(Stat2Data)
library(ggplot2)
data(TextPrices)

p <- qplot(x = Pages, y = Price, data = TextPrices) + 
  ylab("Textbook price") + 
  xlab("No. of pages")
print(p)
```

<br>
<br>
<br>
<br>
Need to choose $f(x)$


## Challenges we will face {.columns-2}

### Model specification
<br>
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(fpp)
library(forecast)
data(ausbeer)

ausbeer.df <- data.frame(quarter = as.numeric(time(ausbeer)), production = as.numeric(ausbeer))

ausbeer.arima <- auto.arima(ausbeer)
ausbeer.df$arima <- as.numeric(fitted(ausbeer.arima))

qplot(x = quarter, y = production, data = ausbeer.df) + 
  geom_line(aes(y = arima), colour = I("#377eb8"), alpha = I(0.8)) +
  geom_smooth(se = FALSE, colour = I("#e41a1c"), alpha = I(0.8)) + 
  ylab("Beer production") + 
  xlab("Year")
```

<br>
<br>
<br>
<br>
Need to choose $f(x)$


## Challenges we will face {.columns-2}

### Model fitting
```{r, echo=FALSE, message=FALSE}

library(MASS)
rmod <- rlm(Price ~ Pages, data = TextPrices)

p + geom_smooth(method = "lm", se = FALSE, colour = I("#377eb8")) + 
  geom_abline(intercept = coef(rmod)[1], slope = coef(rmod)[2], colour = I("#e41a1c")) + 
  geom_abline(intercept = -10, slope = .15, colour = I("#4daf4a"))

```

<br>
<br>
<br>
<br>
How can we estimate the unknown parameters in our model?

$$f(x) = \beta_0 + \beta_1x$$

What should $\beta_0$ and $\beta_1$ be?


## Challenges we will face

### Dealing with randomness
If our model is 

$$Y_i = f(X_{i1}, X_{i2}, \ldots, X_{ik}) + \varepsilon$$

what distribution should $\varepsilon$ follow?


## What are we going to do in Math 430?

### Model
- Learn the statistical modeling thought process
    1. Stating the problem in non-statistical terms
    2. Assessment of data quality
    3. Choose (specify) the model
    4. Fit the model
    5. Assess (validate) the model
    6. Use the model to answer the question(s)
- We will focus on regression models
    
## Logistics

### Textbook
- *A Modern Approach to Regression with R* by Simon J. Sheather, Springer, 2009
- One copy on 2-hour reserve at the Mudd

### Class sessions
- 2 days per week will focus on learning new methods
- Real examples emphasized

### Lab sessions
- 1 day each week will focus on making sure that you are comfortable with using R for statistical modeling


## Grading

### Homework (50%)
- Approximately weekly assignments
- Typically due Wednesdays by 4:30
- No late work 

### Midterm (20%)
- To ensure that everyone is keeping up with the material

### Final project (30%)
- Carryout an entire analysis of data set you choose
- More information later

## Overview {.columns-2}

### Topic
1. Simple linear regression
2. Multiple linear regression
3. Logistic regression

### Chapters
- 2--4
- 5--7
- 8

## A famous motivating example


<!-- <img class=center src=fig/galton.jpg height=150> -->

### (Perhaps surprisingly, this example is still relevant)

<!-- <img class=center src=fig/height.png height=150> -->

[http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html](http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html)

[Predicting height: the Victorian approach beats modern genomics](http://www.wired.com/wiredscience/2009/03/predicting-height-the-victorian-approach-beats-modern-genomics/)

---
## Questions for this class
* Consider trying to answer the following kinds of questions:
  * To use the parents' heights to predict childrens' heights.
  * To try to find a parsimonious, easily described mean 
    relationship between parent and children's heights.
  * To investigate the variation in childrens' heights that appears 
  unrelated to parents' heights (residual variation).
  * To quantify what impact genotype information has beyond parental height in explaining child height.
  * To figure out how/whether and what assumptions are needed to
    generalize findings beyond the data in question.  
  * Why do children of very tall parents tend to be 
    tall, but a little shorter than their parents and why children of very short parents tend to be short, but a little taller than their parents? (This is a famous question called 'Regression to the mean'.)

---
## Galton's Data

* Let's look at the data first, used by Francis Galton in 1885. 
* Galton was a statistician who invented the term and concepts
  of regression and correlation, founded the journal Biometrika,
  and was the cousin of Charles Darwin.
* You may need to run `install.packages("UsingR")` if the `UsingR` library is not installed.
* Let's look at the marginal (parents disregarding children and children disregarding parents) distributions first. 
  * Parent distribution is all heterosexual couples.
  * Correction for gender via multiplying female heights by 1.08.
  * Overplotting is an issue from discretization.

---
## Code

```{r galton,fig.height=3.5,fig.width=8}
library(UsingR); data(galton)
par(mfrow=c(1,2))
hist(galton$child,col="blue",breaks=100)
hist(galton$parent,col="blue",breaks=100)
```

---
## Finding the middle via least squares
* Consider only the children's heights. 
  * How could one describe the "middle"?
  * One definition, let $Y_i$ be the height of child $i$ for $i = 1, \ldots, n = 928$, then define the middle as the value of $\mu$
  that minimizes $$\sum_{i=1}^n (Y_i - \mu)^2$$
* This is physical center of mass of the histrogram.
* You might have guessed that the answer $\mu = \bar X$.


---
## Experiment
### Use R studio's manipulate to see what value of $\mu$ minimizes the sum of the squared deviations.

```
library(manipulate)
myHist <- function(mu){
  hist(galton$child,col="blue",breaks=100)
  lines(c(mu, mu), c(0, 150),col="red",lwd=5)
  mse <- mean((galton$child - mu)^2)
  text(63, 150, paste("mu = ", mu))
  text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```

---
## The least squares estimate is the empirical mean
```{r lsm, dependson="galton",fig.height=4,fig.width=4}
  hist(galton$child,col="blue",breaks=100)
  meanChild <- mean(galton$child)
  lines(rep(meanChild,100),seq(0,150,length=100),col="red",lwd=5)
```

---
### The math follows as:
$$ 
\begin{align} 
\sum_{i=1}^n (Y_i - \mu)^2 & = \
\sum_{i=1}^n (Y_i - \bar Y + \bar Y - \mu)^2 \\ 
& = \sum_{i=1}^n (Y_i - \bar Y)^2 + \
2 \sum_{i=1}^n (Y_i - \bar Y)  (\bar Y - \mu) +\
\sum_{i=1}^n (\bar Y - \mu)^2 \\
& = \sum_{i=1}^n (Y_i - \bar Y)^2 + \
2 (\bar Y - \mu) \sum_{i=1}^n (Y_i - \bar Y)  +\
\sum_{i=1}^n (\bar Y - \mu)^2 \\
& = \sum_{i=1}^n (Y_i - \bar Y)^2 + \
2 (\bar Y - \mu)  (\sum_{i=1}^n Y_i - n \bar Y) +\
\sum_{i=1}^n (\bar Y - \mu)^2 \\
& = \sum_{i=1}^n (Y_i - \bar Y)^2 + \sum_{i=1}^n (\bar Y - \mu)^2\\ 
& \geq \sum_{i=1}^n (Y_i - \bar Y)^2 \
\end{align} 
$$

---
## Comparing childrens' heights and their parents' heights

```{r, dependson="galton",fig.height=4,fig.width=4}
plot(galton$parent,galton$child,pch=19,col="blue")
```

---
Size of point represents number of points at that (X, Y) combination (See the Rmd file for the code).

```{r freqGalton, dependson="galton",fig.height=6,fig.width=6,echo=FALSE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
plot(as.numeric(as.vector(freqData$parent)), 
     as.numeric(as.vector(freqData$child)),
     pch = 21, col = "black", bg = "lightblue",
     cex = .15 * freqData$freq, 
     xlab = "parent", ylab = "child")
```

---
## Regression through the origin
* Suppose that $X_i$ are the parents' heights.
* Consider picking the slope $\beta$ that minimizes $$\sum_{i=1}^n (Y_i - X_i \beta)^2$$
* This is exactly using the origin as a pivot point picking the
line that minimizes the sum of the squared vertical distances
of the points to the line
* Use R studio's  manipulate function to experiment
* Subtract the means so that the origin is the mean of the parent
and children's heights

---
```
myPlot <- function(beta){
  y <- galton$child - mean(galton$child)
  x <- galton$parent - mean(galton$parent)
  freqData <- as.data.frame(table(x, y))
  names(freqData) <- c("child", "parent", "freq")
  plot(
    as.numeric(as.vector(freqData$parent)), 
    as.numeric(as.vector(freqData$child)),
    pch = 21, col = "black", bg = "lightblue",
    cex = .15 * freqData$freq, 
    xlab = "parent", 
    ylab = "child"
    )
  abline(0, beta, lwd = 3)
  points(0, 0, cex = 2, pch = 19)
  mse <- mean( (y - beta * x)^2 )
  title(paste("beta = ", beta, "mse = ", round(mse, 3)))
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
```

---
## The solution 
### In the next few lectures we'll talk about why this is the solution
```{r}
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
```

---
## Visualizing the best fit line
### Size of points are frequencies at that X, Y combination
```{r, fig.height=5,fig.width=5,echo=FALSE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
plot(as.numeric(as.vector(freqData$parent)), 
     as.numeric(as.vector(freqData$child)),
     pch = 21, col = "black", bg = "lightblue",
     cex = .05 * freqData$freq, 
     xlab = "parent", ylab = "child")
lm1 <- lm(galton$child ~ galton$parent)
lines(galton$parent,lm1$fitted,col="red",lwd=3)
```
